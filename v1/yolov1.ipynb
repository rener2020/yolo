{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Predictions Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system divides the input image into a S Ã— S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. \n",
    "\n",
    "æ¯å¼ å›¾åƒåˆ†æˆ7x7çš„gridï¼Œæ¯ä¸ªgridéƒ½ç”¨äºç›®æ ‡æ£€æµ‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts.\n",
    "\n",
    "æ¯ä¸ªgridé¢„æµ‹Bä¸ªbounding boxï¼Œæ¯ä¸ªbounding boxç±»åˆ«ä¸ç½®ä¿¡åº¦\n",
    "$$confidence = Pr(object)*IOU_{pred}^{truth}$$\n",
    "\n",
    "https://hackernoon.com/understanding-yolo-f5a74bbc7967\n",
    "Each grid cell predicts B bounding boxes as well as C class probabilities. The bounding box prediction has 5 components: (x, y, w, h, confidence)  \n",
    "æ¯ä¸ªgrid cellé¢„æµ‹Bä¸ªåŒ…å›´ç›’ï¼Œå…¶ä¸­æœ‰Cç±»åˆ«æ¦‚ç‡ï¼ŒåŒ…å›´ç›’æœ‰äº”ä¸ªéƒ¨åˆ† (x, y, w, h, confidence)\n",
    "\n",
    "The (x, y) coordinates represent the center of the box, relative to the grid cell location (remember that, if the center of the box does not fall inside the grid cell, than this cell is not responsible for it). These coordinates are normalized to fall between 0 and 1. The (w, h) box dimensions are also normalized to [0, 1], relative to the image size. Letâ€™s look at an example:  \n",
    "(x, y)åæ ‡è¡¨ç¤ºäº†ç›’å­çš„ç›¸å¯¹äºgrid cellçš„ä¸­å¤®ï¼Œå¹¶ä¸”ï¼Œè¯¥åæ ‡ç›¸å½“äºç½‘æ ¼å¤§å°å½’ä¸€åŒ–ï¼Œ(w, h)ç›¸å¯¹äºå›¾åƒå¤§å°å½’ä¸€åŒ–ï¼Œä¸¤ä¸ªå½’ä¸€åŒ–éƒ½å½’ä¸€åŒ–åˆ°[0,1]ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"./images/01.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the confidence reflects the presence or absence of an object of any class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the 5 components of the box prediction, remember that each grid cell makes B of those predictions, so there are in total S x S x B * 5 outputs related to bounding box predictions.  \n",
    "ä¸€å…±æœ‰$S\\times{}S\\times{}B\\times{}5$ä¸ªé¢„æµ‹å€¼ä¸è¾¹ç•Œæ¡†çš„é¢„æµ‹ç›¸å…³ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also necessary to predict the class probabilities, Pr(Class(i) | Object). This probability is conditioned on the grid cell containing one object (see this if you donâ€™t know that conditional probability means). In practice, it means that if no object is present on the grid cell, the loss function will not penalize it for a wrong class prediction, as we will see later. The network only predicts one set of class probabilities per cell, regardless of the number of boxes B. That makes S x S x C class probabilities in total  \n",
    "è¿™é‡Œçš„Pr(Class(i) | Object)æ˜¯æ¡ä»¶æ¦‚ç‡ã€‚å¦‚æœä¸€ä¸ªå•å…ƒæ ¼æ²¡æœ‰å¯¹è±¡å­˜åœ¨ï¼ŒæŸå¤±å‡½æ•°ä¸ä¼šå› ä¸ºé”™è¯¯çš„ç±»é¢„æµ‹è€Œæƒ©ç½šä»–ã€‚ä¸è®ºBå€¼ä¸ºå¤šå°‘ï¼Œæ¯ä¸ªç½‘æ ¼åªé¢„æµ‹ä¸€ç»„ç±»æ¦‚ç‡ï¼Œæœ€ç»ˆä¼šç”ŸæˆS x S x Cä¸ªç±»æ¦‚ç‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the class predictions to the output vector, we get a S x S x (B * 5 +C) tensor as output.  \n",
    "æœ€ç»ˆä¼šæœ‰S x S x (B * 5 +C)çš„å¼ é‡è¾“å‡ºã€‚  \n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/output.png\" width=\"600\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The network structure looks like a normal CNN, with convolutional and max pooling layers, followed by 2 fully connected layers in the end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"./images/main_network.png\">\n",
    "<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss function\n",
    "Part 1\n",
    "$$\\lambda_{coord}\\sum_{i=0}^{S^2}\\sum_{j=0}^{B}\\mathbb{1}_{ij}^{obj}{[(x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation computes the loss related to the predicted bounding box position (x,y). Donâ€™t worry about Î» for now, just consider it a given constant. The function computes a sum over each bounding box predictor (j = 0.. B) of each grid cell (i = 0 .. S^2). ğŸ™ obj is defined as follows:  \n",
    "- 1, If an object is present in grid cell i and the jth bounding box predictor is â€œresponsibleâ€ for that prediction\n",
    "- 0, otherwise  \n",
    "\n",
    "è¿™ä¸€éƒ¨åˆ†è®¡ç®—è¾¹ç•Œä½ç½®æ¡†çš„æŸå¤±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be â€œresponsibleâ€ for predicting an object based on which prediction has the highest current IOU with the ground truth.  \n",
    "YOLOä½¿ç”¨IOUæœ€å¤§çš„è¾¹ç•Œæ¡†è¿›è¡Œå›å½’é¢„æµ‹\n",
    "\n",
    "The other terms in the equation should be easy to understand: (x, y) are the predicted bounding box position and (xÌ‚, Å·) hat are the actual position from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "$$\\lambda_{coord}\\sum_{i=0}^{S^2}\\sum_{j=0}^{B}\\mathbb{1}_{ij}^{obj}{[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2+(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the loss related to the predicted box width / height. The equation looks similar to the first one, except for the square root. Whatâ€™s up with that? Quoting the paper again:\n",
    "Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.  \n",
    "ä½¿ç”¨æ ¹å·æ”¾å¤§è¾ƒå°çš„è¾¹ç•Œæ¡†ï¼ŒèƒŒåçš„å«ä¹‰æ˜¯ï¼šå°è¾¹ç•Œæ¡†ä¸Šçš„å°è¯¯å·®åº”è¯¥å¤§äºå¤§è¾¹ç•Œæ¡†ä¸Šçš„å°è¯¯å·®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MainModel, self).__init__()\n",
    "\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            # Conv 1 7x7x64 s=2\n",
    "            torch.nn.Conv2d(3, 64, (7, 7), 2, padding=(3, 3)),  # 64\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            # Max Pool 1\n",
    "            torch.nn.MaxPool2d((2, 2), 2),  # 64\n",
    "            # Conv 2\n",
    "            torch.nn.Conv2d(64, 192, (3, 3), padding=(1,1)),  # 192\n",
    "            # Max Pool 2\n",
    "            torch.nn.MaxPool2d((2, 2), 2),  # 192\n",
    "            # Conv 3\n",
    "            torch.nn.Conv2d(192, 128, (1, 1)),  # 128\n",
    "            # Conv 4\n",
    "            torch.nn.Conv2d(128, 256, (3, 3), padding=(1,1)),  # 256\n",
    "            # Conv 5\n",
    "            torch.nn.Conv2d(256, 256, (1, 1)),  # 256\n",
    "            # Conv 6\n",
    "            torch.nn.Conv2d(256, 512, (1, 1)),  # 512\n",
    "            # Max Pool 3\n",
    "            torch.nn.MaxPool2d((2, 2), 2),  # 512\n",
    "            # Conv 7\n",
    "            torch.nn.Conv2d(512, 256, (1, 1)),  # 256\n",
    "            # Conv 8\n",
    "            torch.nn.Conv2d(256, 512, (3, 3), padding=(1,1)),  # 512\n",
    "            # Conv 9\n",
    "            torch.nn.Conv2d(512, 256, (1, 1)),  # 256\n",
    "            # Conv 10\n",
    "            torch.nn.Conv2d(256, 512, (3, 3), padding=(1,1)),  # 512\n",
    "            # Conv 11\n",
    "            torch.nn.Conv2d(512, 256, (1, 1)),  # 256\n",
    "            # Conv 12\n",
    "            torch.nn.Conv2d(256, 512, (3, 3), padding=(1,1)),  # 512\n",
    "            # Conv 13\n",
    "            torch.nn.Conv2d(512, 256, (1, 1)),  # 256\n",
    "            # Conv 14\n",
    "            torch.nn.Conv2d(256, 512, (3, 3), padding=(1,1)),  # 512\n",
    "            # Conv 15\n",
    "            torch.nn.Conv2d(512, 512, (1, 1)),  # 512\n",
    "            # Conv 16\n",
    "            torch.nn.Conv2d(512, 1024, (3, 3), padding=(1,1)),  # 1024\n",
    "            # Max Pool 4\n",
    "            torch.nn.MaxPool2d((2, 2), 2),  # 1024\n",
    "            # Conv 17\n",
    "            torch.nn.Conv2d(1024, 512, (1, 1)),  # 512\n",
    "            # Conv 18\n",
    "            torch.nn.Conv2d(512, 1024, (3, 3), padding=(1,1)),  # 1024\n",
    "            # Conv 19\n",
    "            torch.nn.Conv2d(1024, 512, (1, 1)),  # 512\n",
    "            # Conv 20\n",
    "            torch.nn.Conv2d(512, 1024, (3, 3), padding=(1,1)),  # 1024\n",
    "            # Conv 21\n",
    "            torch.nn.Conv2d(1024, 1024, (3, 3), padding=(1,1)),  # 1024\n",
    "            # Conv 22\n",
    "            torch.nn.Conv2d(1024, 1024, (3, 3), 2, padding=(1,1)),  # 1024\n",
    "            # Conv 23\n",
    "            torch.nn.Conv2d(1024, 1024, (3, 3), padding=(1,1)),  # 1024\n",
    "            # Conv 24\n",
    "            torch.nn.Conv2d(1024, 1024, (3, 3), padding=(1,1)),  # 1024\n",
    "            # FC1\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(7*7*1024, 4096),  # 4096\n",
    "            # FC2\n",
    "            torch.nn.Linear(4096, 1470),  # 7x7x30=1470\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pred(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = MainModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,472\n",
      "         MaxPool2d-2         [-1, 64, 112, 112]               0\n",
      "            Conv2d-3        [-1, 192, 112, 112]         110,784\n",
      "         MaxPool2d-4          [-1, 192, 56, 56]               0\n",
      "            Conv2d-5          [-1, 128, 56, 56]          24,704\n",
      "            Conv2d-6          [-1, 256, 56, 56]         295,168\n",
      "            Conv2d-7          [-1, 256, 56, 56]          65,792\n",
      "            Conv2d-8          [-1, 512, 56, 56]         131,584\n",
      "         MaxPool2d-9          [-1, 512, 28, 28]               0\n",
      "           Conv2d-10          [-1, 256, 28, 28]         131,328\n",
      "           Conv2d-11          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-12          [-1, 256, 28, 28]         131,328\n",
      "           Conv2d-13          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-14          [-1, 256, 28, 28]         131,328\n",
      "           Conv2d-15          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-16          [-1, 256, 28, 28]         131,328\n",
      "           Conv2d-17          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-18          [-1, 512, 28, 28]         262,656\n",
      "           Conv2d-19         [-1, 1024, 28, 28]       4,719,616\n",
      "        MaxPool2d-20         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-21          [-1, 512, 14, 14]         524,800\n",
      "           Conv2d-22         [-1, 1024, 14, 14]       4,719,616\n",
      "           Conv2d-23          [-1, 512, 14, 14]         524,800\n",
      "           Conv2d-24         [-1, 1024, 14, 14]       4,719,616\n",
      "           Conv2d-25         [-1, 1024, 14, 14]       9,438,208\n",
      "           Conv2d-26           [-1, 1024, 7, 7]       9,438,208\n",
      "           Conv2d-27           [-1, 1024, 7, 7]       9,438,208\n",
      "           Conv2d-28           [-1, 1024, 7, 7]       9,438,208\n",
      "          Flatten-29                [-1, 50176]               0\n",
      "           Linear-30                 [-1, 4096]     205,524,992\n",
      "           Linear-31                 [-1, 1470]       6,022,590\n",
      "================================================================\n",
      "Total params: 270,654,974\n",
      "Trainable params: 270,654,974\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 121.01\n",
      "Params size (MB): 1032.47\n",
      "Estimated Total Size (MB): 1155.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(my_nn,(3, 448, 448))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some comments about the architecture:**\n",
    "- Note that the architecture was crafted for use in the Pascal VOC dataset, where the authors used S=7, B=2 and C=20. This explains why the final feature maps are 7x7, and also explains the size of the output (7x7x(2*5+20)). Use of this network with a different grid size or different number of classes might require tuning of the layer dimensions.  \n",
    "è¿™ä¸ªæ¶æ„æ˜¯ä¸“é—¨è®¾è®¡ç»™VOCæ•°æ®é›†çš„ï¼ŒVOCæ•°æ®é›†é‡Œé¢çš„å›¾ç‰‡æ˜¯448x448çš„ï¼Œ448/7=64ï¼Œä½¿ç”¨å…¶ä»–åˆ†è¾¨ç‡çš„å›¾ç‰‡æœ€å¥½è°ƒæ•´ç½‘ç»œç»“æ„\n",
    "- The authors mention that there is a fast version of YOLO, with fewer convolutional layers. The table above, however, display the full version.\n",
    "- The sequences of 1x1 reduction layers and 3x3 convolutional layers were inspired by the GoogLeNet (Inception) model  \n",
    "1x1å·ç§¯å±‚\n",
    "- The final layer uses a linear activation function. All other layers use a leaky\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1 Convolution\n",
    "The 1x1 convolution operation involves convoling the inputs with filters of size 1x1, usually with zero-padding and stirde of 1.  \n",
    "A general-purpose convolutional layer which outputs a tensor of shape (B,K,H,W) where,\n",
    "- B : Batch Size\n",
    "- K : The number of convolutional filters of kernels\n",
    "- H : Height\n",
    "- W : Width\n",
    "changing filter dimension from K to F.\n",
    "## Benefits?\n",
    "- Small-sized filters make a low number of parameters\n",
    "- Spatial downsampling (though pooling) may cause information loss, 1x1 convolution can be a alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
